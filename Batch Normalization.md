# Batch Normalization
> BN就是调整每层网络输出数据的分布，使其进入激活函数的作用区。激活函数的作用区就是指原点附近的区域，梯度弥散率低，区分率高。同时，BN会在训练过程中，自己调节数据分布，使其“更合理”地进入激活函数。

[TOC]

## 用途
- 简化调参,初始化要求降低,可以使用大的学习率
- Batchnorm也是一种正则,类似于L1,L2,Dropout
- 降低了数据之间的绝对差异,考虑相对差异
- 加速收敛，因为不必神经网络去适应数据的分布，完美地使用了激活函数对数据的“修剪”，减少梯度弥散
- 准确率更高，对激活函数充分利用所致

## 用在哪
- 一般用在conv之后,但用在激活函数之前几乎是必须的
- ![20181017151000434](7F06236F2BAF449F9E04C449233F71B0)
  > 当batch特征图分布比较任性(batch比较小的时候，很容易出现任性的情况，如上图前两种情况)。上述用的激活函数是relu，在输入大于0时保持不变，小于0时变为0。对于一二两种情况，要么修剪得过少，等于没用激活函数；要么修剪得过多，一大批0传到后面。当然，在不断训练的过程中，情况一二都会慢慢减少，因为特征提取层越来越牛x。但这需要用更多训练时间去填补，而且最终的性能不会很好，虽然是batch训练，但它的视野没到batch分布那个层面。视野太窄，也更容易过拟合。如果用其他激活函数比如sigmoid/tanh等，还有梯度加速弥散的问题存在。(自己可以画图看看，两端抑制)OK，我们想要的是上图表示的第三种情况，激活函数的修剪率适中。这就需要用到BN操作了。

## 怎么用
![20181017152656169](717F3107E2FD463784920C5C684A25AB)
  - 第一步：获得一个batch里的输入，batch_size=m
  - 第二步：求这个batch的均值μ和方差
  - 第三部：对x标准化，得到x’
  - 第四部：对x’做线性变换，得到输出y
- 第四步的核心是==γ和β是可以学习的==，意味着==神经网络会随着训练过程自己挑选一个最适合的分布==，如果不用γ和β，那么需要训练特征提取层来符合最优分布就是关于y轴的对称曲线，相对于只需要训练两个数，后者训练成本更低